import csv

from bs4 import BeautifulSoup
import requests
import locale
import re

import json
import os
from selenium import webdriver

class  ParserOLd:
    def __init__(self):
        self.log_data = []
        pass

    def agent(self, numer):
        self.num = numer
        try:
            test_url3 = f'https://zakupki.gov.ru/epz/order/notice/ok20/view/common-info.html?regNumber={numer}'
            test_url2 = f'https://zakupki.gov.ru/epz/order/notice/notice223/common-info.html?noticeInfoId={numer}'
            self.HEADERS = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0'
                              ' YaBrowser/23.3.0.2246 Yowser/2.5 Safari/537.36', 'accept': '*/*'}

            req = requests.get(test_url2, headers=self.HEADERS, params=None)
            src = req.text
            self.soup = BeautifulSoup(src, 'lxml')
            self.status = 'Успешное подключение'
            return self.soup
        except:
            self.status = 'Ошибка подключения'

    ##############################################################################################################
    #основная информация
    ######################################################
    #Серийный номер

    def parse_head(self ):
        mass = []
        serial_date = {}
        source = self.soup.find(class_="col-6 pr-0 mr-21px")
        lines = source.get_text().strip().splitlines()
        # Удалите пустые строки
        mainMass = [line.strip() for line in lines if line.strip()]
        sourceleft = self.soup.find(class_="col d-flex flex-column registry-entry__right-block b-left")
        lines = sourceleft.get_text().strip().splitlines()
        leftMass = [line.strip() for line in lines if line.strip()]
        cleaned_left = [item.replace('\xa0', ' ') for item in leftMass]
        return  mainMass+cleaned_left
    
    def get_links(self):
        tabs_of_links = {}
        link_razdels = self.soup.find(class_='container card-layout')
        if link_razdels != None:
            try:
                link_razdels = self.soup.find(class_ = 'tabsNav d-flex').find_all('a')
                for links in link_razdels:
                    linkl = 'https://zakupki.gov.ru/' + links.get('href')
                    title_razde = links.text.strip()
                    tabs_of_links[title_razde] = linkl
                    
            except:
                link_razdels = self.soup.find(class_='tabsNav d-flex align-items-end').find_all('a')
                for links in link_razdels:
                    linkl = 'https://zakupki.gov.ru/' + links.get('href')
                    title_razde = links.text.strip()
                    tabs_of_links[title_razde] = linkl
            return tabs_of_links

    def mainInfo(self):
        mainMass = []
        newmainMass = []
        JornalMass = []
        tabs_of_links = self.get_links()
        for title, link in tabs_of_links.items():
            req = requests.get(url=link, headers=self.HEADERS)
            src = req.text
            soup = BeautifulSoup(src, "lxml")
            if title == 'Журнал событий':
                JornalMass = self.get_jornals(link)
            try:
                
                containerMain = soup.find_all(class_='card-common-content')
                containerMain2 = soup.find_all(class_='card-attachments')
            
                if containerMain != None:
                    for i in containerMain:
                        lines = i.get_text().strip().splitlines()
                        Mass = [line.strip() for line in lines if line.strip()]
                        mainMass =  mainMass + Mass
                
                if containerMain2 != None:
                    for i in containerMain2:
                        lines = i.get_text().strip().splitlines()
                        Mass = [line.strip() for line in lines if line.strip()]
                        newmainMass =  newmainMass + Mass
            except:
                print('Error')
        mainMass.append(newmainMass)
        mainMass.append(JornalMass)
        return mainMass
    
    def get_jornals(self, link):
        JornalMass= []
        driver = webdriver.Chrome()
        driver.get(link)
        import time
        time.sleep(0)
        html = driver.page_source
        soup = BeautifulSoup(html, 'lxml')
        driver.quit()
        containerMain3 = soup.find_all(class_='tabBoxWrapper tabBoxWrapper__mb24')
        if containerMain3 != None:
            for i in containerMain3:
                lines = i.get_text().strip().splitlines()
                Mass = [line.strip() for line in lines if line.strip()]
                JornalMass = JornalMass + Mass
        return JornalMass

par = ParserOLd()
par.agent('5963715')
# print(par.parse_head())
# print(par.get_links())
# print(par.mainInfo())
print(par.get_jornals('https://zakupki.gov.ru/epz/order/notice/notice223/event-journal.html?noticeInfoId=5963715'))